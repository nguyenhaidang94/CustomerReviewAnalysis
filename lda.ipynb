{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z99zdW1ppFlw",
        "LSo5X2s5Szai",
        "HmlHUfpW8GGx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenhaidang94/CustomerReviewAnalysis/blob/master/lda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLAxXj9ENatu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import numpy.random as npr\n",
        "from gensim.models.phrases import Phrases, Phraser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5FW0KSLOQZn",
        "colab_type": "code",
        "outputId": "4eb03dac-1003-4b54-b1a8-6cf6cca5db0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z99zdW1ppFlw",
        "colab_type": "text"
      },
      "source": [
        "## Clone repo and install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqtg8vhUpDW1",
        "colab_type": "code",
        "outputId": "2253fedd-f125-4736-b52f-ae90b52b267f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!git clone https://github.com/deepai-solutions/core_nlp.git\n",
        "!pip install python-crfsuite\n",
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'core_nlp'...\n",
            "remote: Enumerating objects: 205, done.\u001b[K\n",
            "remote: Total 205 (delta 0), reused 0 (delta 0), pack-reused 205\u001b[K\n",
            "Receiving objects: 100% (205/205), 9.69 MiB | 10.46 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "Collecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 6.7MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.6\n",
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.6)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.28.1)\n",
            "Installing collected packages: sklearn-crfsuite\n",
            "Successfully installed sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSo5X2s5Szai",
        "colab_type": "text"
      },
      "source": [
        "### Test tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbXJTPTLpbkz",
        "colab_type": "code",
        "outputId": "c4c7d99d-e079-4e88-8336-3366c5cbc04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from core_nlp.tokenization.crf_tokenizer import CrfTokenizer\n",
        "# Test tokenizer\n",
        "tokenizer = CrfTokenizer(config_root_path='core_nlp/tokenization/',\n",
        "                          model_path='core_nlp/models/pretrained_tokenizer.crfsuite')\n",
        "test_sent = \"Thuế thu nhập cá nhân\"\n",
        "tokenized_sent = tokenizer.get_tokenized(test_sent)\n",
        "print(tokenized_sent)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file core_nlp/models/pretrained_tokenizer.crfsuite\n",
            "Thuế_thu_nhập cá_nhân\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HmlHUfpW8GGx"
      },
      "source": [
        "## First look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "niEe_yl48GGt",
        "colab": {}
      },
      "source": [
        "unlabeled_folder_path = \"/content/drive/My Drive/Master/2nd Semester/AdvMLDM/Project_AdvMLDM/dataset/unlabeled_data\"\n",
        "seperator = \"/\"\n",
        "file_name = \"foody_data.pkl\"\n",
        "\n",
        "def load_data():\n",
        "  return pd.read_pickle(unlabeled_folder_path + seperator + file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4j5b7gkO8GGi"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t8RXZK3x8GGg"
      },
      "source": [
        "### Remove unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d9qbjOSz8GGa",
        "colab": {}
      },
      "source": [
        "def remove_unnecessary_columns(df, columns):\n",
        "  df.drop(columns=columns, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SkozxiNX8GGW"
      },
      "source": [
        "### Remove false mark and not having mark reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXNhn0Gd8GGQ",
        "colab": {}
      },
      "source": [
        "false_mark = ' ́'[1] + ' ̀'[1] + ' ̉'[1] + ' ̣'[1] + ' ̃'[1]\n",
        "vietnam_alp = 'áàãảạăắằẳẵặâấầẩẫậóòỏõọôốồổộỗơớờởỡợúùủũụưứừữựửíìỉĩịýỳỷỹỵéèẻẽẹêếềểễệ'\n",
        "\n",
        "\n",
        "def is_false_mark(string):\n",
        "  \"\"\"\n",
        "    True if string is false mark. \n",
        "  \"\"\"\n",
        "  return (any ( c in false_mark for c in string))\n",
        "    \n",
        "  \n",
        "def is_have_mark(string):\n",
        "  \"\"\"\n",
        "    False if string is not have mark \n",
        "  \"\"\"\n",
        "  return (any ( c in vietnam_alp for c in string))\n",
        "\n",
        "def find_false_mark(df, col):\n",
        "  false_mark_indices = []\n",
        "  for index, row in df.iterrows():\n",
        "    if is_false_mark(row[col].lower()):\n",
        "      false_mark_indices.append(index)\n",
        "  return false_mark_indices\n",
        "\n",
        "def find_not_having_mark(df, col):\n",
        "  not_have_mark_indices = []\n",
        "  for index, row in df.iterrows():\n",
        "    if not is_have_mark(row[col].lower()):\n",
        "      not_have_mark_indices.append(index)\n",
        "  return not_have_mark_indices\n",
        "\n",
        "def remove_false_mark_not_having_mark(df):\n",
        "  false_mark_indices = find_false_mark(df, origin_review_col)\n",
        "  not_have_mark_indices = find_not_having_mark(df, origin_review_col)\n",
        "  print(\"False mark: {}, take {}%\".format(len(false_mark_indices)\n",
        "                                        , len(false_mark_indices)/len(df) * 100))\n",
        "  print(\"Don't have mark: {}, take {}%\".format(len(not_have_mark_indices)\n",
        "                                             , len(not_have_mark_indices)/len(df) * 100))\n",
        "  \n",
        "  removed_indices = np.concatenate((false_mark_indices, not_have_mark_indices))\n",
        "  df.drop(index=removed_indices, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nV6F-Y_u8GGO"
      },
      "source": [
        "### Remove new line, comma, special chars, numbers, special words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0ThqkErH8GGI",
        "colab": {}
      },
      "source": [
        "special_chars_re = \"[+\\-\\\"():;\\\\\\/\\^<>']\"\n",
        "numbers_re = \"\\d+\"\n",
        "\n",
        "# TODO: some reviews, user starts new line without dot\n",
        "def remove_new_line(str):\n",
        "  return str.replace(\"\\n\", \"\")\n",
        "\n",
        "def remove_comma(str):\n",
        "  return str.replace(\",\", \"\")\n",
        "\n",
        "def remove_sepcial_chars(str):\n",
        "  return re.sub(special_chars_re, \"\", str)\n",
        "\n",
        "def remove_numbers(str):\n",
        "  return re.sub(numbers_re, \"\", str)\n",
        "\n",
        "def remove_special_word(str, special_word):\n",
        "  return str.replace(special_word, \"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LDW3sTCH8GGG"
      },
      "source": [
        "### Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wuk7rhcu8GGD",
        "colab": {}
      },
      "source": [
        "def read_file_content(path):\n",
        "  with open(path, 'r') as f:\n",
        "    return f.read()\n",
        "def remove_stop_words(str, stop_words):\n",
        "  words = str.split()\n",
        "  result_words  = [word for word in words if word not in stop_words]\n",
        "  return ' '.join(result_words)\n",
        "def remove_stop_words_in_list(list_str, stop_words):\n",
        "  resutl_str = []\n",
        "  for str in list_str:\n",
        "    resutl_str.append(remove_stop_words(str, stop_words))\n",
        "  return resutl_str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZFIdhY38GF7"
      },
      "source": [
        "### Lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaHM3R2l8GF2",
        "colab": {}
      },
      "source": [
        "def lower_case(str):\n",
        "  return str.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sEAjceGL8GF0"
      },
      "source": [
        "### Split sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qFd1js0R8GFq",
        "colab": {}
      },
      "source": [
        "def split_sentence(str):\n",
        "  sentences = re.split(\"\\.|!|\\?|;\", str)\n",
        "  # remove empty sentence\n",
        "  sentences = list(filter(None, sentences))\n",
        "  # remove space from start and end of string\n",
        "  for i in range(0, len(sentences)):\n",
        "    sentences[i] = sentences[i].strip()\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xOJXstL-8GFn"
      },
      "source": [
        "### Combine words using gensim Phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kM7oyN2C8GFd",
        "colab": {}
      },
      "source": [
        "def collect_sentences(df):\n",
        "  all_sentences = []\n",
        "  for index, row in df.iterrows():\n",
        "    sentences = split_sentence(df.loc[index, review_col])\n",
        "    for sentence in sentences:\n",
        "      all_sentences.append(sentence)\n",
        "  return all_sentences\n",
        "\n",
        "def train_bigram_model(df, min_count):\n",
        "  print(\"Split sentences...\")\n",
        "  sentences = collect_sentences(df)\n",
        "  print(\"Split words...\")\n",
        "  words = [sentence.split() for sentence in sentences]\n",
        "  print(\"Train bi-gram model...\")\n",
        "  phrases = Phrases(words, min_count=min_count)\n",
        "  bigram = Phraser(phrases)\n",
        "  return bigram\n",
        "\n",
        "def combine_words(review, bigram_model):\n",
        "  words = review.split()\n",
        "  new_review = \" \".join(bigram_model[words])\n",
        "  return new_review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XnfufYcr8GFc"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1RoYbtw78GFX",
        "colab": {}
      },
      "source": [
        "def tokenize(list_str, tokenizer):\n",
        "  list_tokens = []\n",
        "  for str in list_str:\n",
        "    tokens = []\n",
        "    if str != \"\":\n",
        "      tokens = tokenizer.tokenize(str)\n",
        "    list_tokens.append(tokens)\n",
        "  return list_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vWsUCdha8GFR",
        "colab": {}
      },
      "source": [
        "# Tokenizing n sentences\n",
        "def tokenize_data(df, tokenizer, n_sentences):\n",
        "  completed_sentences = 0\n",
        "  all_tokens = []\n",
        "  for index, row in df.iterrows():\n",
        "    sentences = df.loc[index, sentences_col]\n",
        "    tokens = tokenize(sentences, tokenizer)\n",
        "    for token in tokens:\n",
        "      all_tokens.append(token)\n",
        "    completed_sentences += len(sentences)\n",
        "    if (completed_sentences >= n_sentences):\n",
        "      break\n",
        "\n",
        "  return all_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bgWwrYb8GFK",
        "colab": {}
      },
      "source": [
        "def collect_tokens(df, n_sentences):\n",
        "  all_tokens = []\n",
        "  completed_sentences = 0\n",
        "  for index, row in df.iterrows():\n",
        "    tokens = df.loc[index, tokens_col]\n",
        "    for token in tokens:\n",
        "      all_tokens.append(token)\n",
        "    completed_sentences += len(tokens)\n",
        "    if (completed_sentences >= n_sentences):\n",
        "      break\n",
        "\n",
        "  return all_tokens\n",
        "\n",
        "def collect_all_tokens(df):\n",
        "  all_tokens = []\n",
        "  completed_sentences = 0\n",
        "  for index, row in df.iterrows():\n",
        "    list_tokens = df.loc[index, tokens_col]\n",
        "    for tokens in list_tokens:\n",
        "      all_tokens.append(tokens)\n",
        "\n",
        "  return all_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ojt1f5g8GFF"
      },
      "source": [
        "### Combine all steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTpt2T_m8GE7",
        "colab": {}
      },
      "source": [
        "unnecessary_columns = [\"time\", \"user_name\", \"user_link\", \"review_link\"\n",
        "                       , \"avg_score\", \"location_point\", \"space_point\"\n",
        "                       , \"quality_point\", \"service_point\", \"price_point\"]\n",
        "origin_review_col = \"review_content\"\n",
        "review_col = \"review_cleaned\"\n",
        "special_word = \"Xem thêm\"\n",
        "file_stop_words = \"/content/drive/My Drive/Master/2nd Semester/AdvMLDM/Project_AdvMLDM/vietnamese-stopwords.txt\"\n",
        "sentences_col = \"sentences\"\n",
        "sentences_cleaned_col = \"sentences_cleaned\"\n",
        "tokens_col = \"tokens\"\n",
        "tokenizer = CrfTokenizer(config_root_path='core_nlp/tokenization/',\n",
        "                          model_path='core_nlp/models/pretrained_tokenizer.crfsuite')\n",
        "bigram_model_file = \"/content/drive/My Drive/Master/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/bigram.dat\"\n",
        "bigram_min_count = 20\n",
        "\n",
        "def preprocessing(df):\n",
        "  print(\"Remove unnecessary columns...\")\n",
        "  remove_unnecessary_columns(df, unnecessary_columns)\n",
        "  \n",
        "  print(\"Removing false mark and not having mark reviews...\")\n",
        "  remove_false_mark_not_having_mark(df)\n",
        "  print(\"Number of remaining reviews: {}\".format(len(df)))\n",
        "  \n",
        "  # duplicate review to create column review_cleaned\n",
        "  df[review_col] = df[origin_review_col].values\n",
        "  \n",
        "  print(\"Remove new line...\")\n",
        "  df[review_col] = df[review_col].apply(remove_new_line)\n",
        "  \n",
        "  print(\"Remove comma...\")\n",
        "  data[review_col] = data[review_col].apply(remove_comma)\n",
        "  \n",
        "  print(\"Remove special chars...\")\n",
        "  data[review_col] = data[review_col].apply(remove_sepcial_chars)\n",
        "  \n",
        "  print(\"Remove numbers...\")\n",
        "  data[review_col] = data[review_col].apply(remove_numbers)\n",
        "  \n",
        "  print(\"Remove special words...\")\n",
        "  data[review_col] = data[review_col].apply(remove_special_word, args=(special_word,))\n",
        "  \n",
        "  print(\"Lower case...\")\n",
        "  df[review_col] = df[review_col].apply(lower_case)\n",
        "  \n",
        "#   print(\"Train bigram model...\")\n",
        "#   bigram = train_bigram_model(df, bigram_min_count)\n",
        "#   print(\"Save bigram model...\")\n",
        "#   bigram.save(bigram_model_file)\n",
        "\n",
        "#   print(\"Load bigram model...\")\n",
        "#   bigram = Phrases.load(bigram_model_file)\n",
        "\n",
        "#   print(\"Combine words...\")\n",
        "#   df[review_col] = df[review_col].apply(combine_words, args=(bigram,))\n",
        "  \n",
        "  print(\"Split sentences...\")\n",
        "  df[sentences_col] = df[review_col].apply(split_sentence)\n",
        "  \n",
        "  print(\"Remove stop words...\")\n",
        "  full_stop_words = read_file_content(file_stop_words)\n",
        "  list_stop_words = full_stop_words.split(\"\\n\")\n",
        "  df[sentences_cleaned_col] = df[sentences_col].apply(remove_stop_words_in_list, args=(list_stop_words,))\n",
        "  \n",
        "  print(\"Tokenizing...\")\n",
        "  df[tokens_col] = df[sentences_cleaned_col].apply(tokenize, args=(tokenizer,))\n",
        "  \n",
        "  print(\"Finish preprocessing\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vxu3imFC8GE5"
      },
      "source": [
        "### Run preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzNQZDKf8GEt",
        "colab": {}
      },
      "source": [
        "data = load_data()\n",
        "preprocessing(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yw7bTFiQ_hQZ",
        "colab": {}
      },
      "source": [
        "# save cleaned data\n",
        "data_saved_path = \"drive/My Drive/Master/JVN/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/cleaned_data.pkl\"\n",
        "data.to_pickle(data_saved_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o4ghgXi88GEe"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iVryi1N_8GEY",
        "colab": {}
      },
      "source": [
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.models import LdaMulticore\n",
        "from gensim.models import LdaModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFmKqyR28GET",
        "colab": {}
      },
      "source": [
        "def save_dict(dictionary, path):\n",
        "  dictionary.save_as_text(path)\n",
        "  \n",
        "def save_lda_model(model, path):\n",
        "  model.save(path)\n",
        "  \n",
        "def load_dict(path):\n",
        "  return Dictionary.load_from_text(path)\n",
        "  \n",
        "def load_lda_model(path):\n",
        "  return LdaModel.load(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "reOEiB788GED",
        "colab": {}
      },
      "source": [
        "def rank_topic(tuples):\n",
        "  return sorted(tuples, key=lambda t: t[1], reverse = True)[0][0]\n",
        "\n",
        "def get_topic(topic_mapping, topic):\n",
        "  return topic_mapping.get(topic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4OnBmgr68GD7",
        "colab": {}
      },
      "source": [
        "# load cleand data\n",
        "data_saved_path = \"drive/My Drive/Master/JVN/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/cleaned_data.pkl\"\n",
        "data = pd.read_pickle(data_saved_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq6PEKLFKvCb",
        "colab_type": "code",
        "outputId": "ad2d0ec0-acf8-4790-ddab-91722e872584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>brand_name</th>\n",
              "      <th>brand_link</th>\n",
              "      <th>review_content</th>\n",
              "      <th>review_cleaned</th>\n",
              "      <th>sentences</th>\n",
              "      <th>sentences_cleaned</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>El Sol - Meat &amp; Wine - Võ Thị Sáu</td>\n",
              "      <td>https://www.foody.vn/ho-chi-minh/el-sol-meat-w...</td>\n",
              "      <td>Quán steak hiếm hoi mà mình cực kì ưng ý từ lâ...</td>\n",
              "      <td>quán steak hiếm hoi mà mình cực kì ưng ý từ lâ...</td>\n",
              "      <td>[quán steak hiếm hoi mà mình cực kì ưng ý từ l...</td>\n",
              "      <td>[quán steak hiếm hoi mình cực kì ưng ý lâu nay...</td>\n",
              "      <td>[[quán, steak, hiếm_hoi, mình, cực_kì, ưng_ý, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Busan Korean Food - Món Hàn Quốc - Đinh Tiên H...</td>\n",
              "      <td>https://www.foody.vn/ho-chi-minh/busan-korean-...</td>\n",
              "      <td>Vị trí dễ tìm. Giữ xe rất nhiệt tình. Dắt xe v...</td>\n",
              "      <td>vị trí dễ tìm. giữ xe rất nhiệt tình. dắt xe v...</td>\n",
              "      <td>[vị trí dễ tìm, giữ xe rất nhiệt tình, dắt xe ...</td>\n",
              "      <td>[vị trí dễ tìm, giữ xe nhiệt tình, dắt xe khác...</td>\n",
              "      <td>[[vị_trí, dễ, tìm], [giữ, xe, nhiệt_tình], [dắ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TocoToco Bubble Tea - Cộng Hòa</td>\n",
              "      <td>https://www.foody.vn/ho-chi-minh/tocotoco-bubb...</td>\n",
              "      <td>Địa điểm quán dễ tìm. Không gian cũng rộng, có...</td>\n",
              "      <td>địa điểm quán dễ tìm. không gian cũng rộng có ...</td>\n",
              "      <td>[địa điểm quán dễ tìm, không gian cũng rộng có...</td>\n",
              "      <td>[địa điểm quán dễ tìm, gian rộng lầu view đẹp ...</td>\n",
              "      <td>[[địa_điểm, quán, dễ, tìm], [gian, rộng, lầu, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Le Castella Viet Nam - Bánh Bông Lan Đài Loan ...</td>\n",
              "      <td>https://www.foody.vn/ho-chi-minh/le-castella-v...</td>\n",
              "      <td>Thấy bánh này đang hot rần rần mình cũng gọi t...</td>\n",
              "      <td>thấy bánh này đang hot rần rần mình cũng gọi t...</td>\n",
              "      <td>[thấy bánh này đang hot rần rần mình cũng gọi ...</td>\n",
              "      <td>[thấy bánh hot rần rần mình gọi thử hộp bánh b...</td>\n",
              "      <td>[[thấy, bánh, hot, rần_rần, mình, gọi, thử, hộ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tabletop - Boardgame &amp; Coffee</td>\n",
              "      <td>https://www.foody.vn/ho-chi-minh/tabletop-boar...</td>\n",
              "      <td>Mỗi lần nghĩ tới boardgame thì sẽ nghĩ tới vô ...</td>\n",
              "      <td>mỗi lần nghĩ tới boardgame thì sẽ nghĩ tới vô ...</td>\n",
              "      <td>[mỗi lần nghĩ tới boardgame thì sẽ nghĩ tới vô...</td>\n",
              "      <td>[lần nghĩ tới boardgame nghĩ tới vô, khoái món...</td>\n",
              "      <td>[[lần, nghĩ, tới, boardgame, nghĩ, tới, vô], [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          brand_name  ...                                             tokens\n",
              "0                  El Sol - Meat & Wine - Võ Thị Sáu  ...  [[quán, steak, hiếm_hoi, mình, cực_kì, ưng_ý, ...\n",
              "1  Busan Korean Food - Món Hàn Quốc - Đinh Tiên H...  ...  [[vị_trí, dễ, tìm], [giữ, xe, nhiệt_tình], [dắ...\n",
              "2                     TocoToco Bubble Tea - Cộng Hòa  ...  [[địa_điểm, quán, dễ, tìm], [gian, rộng, lầu, ...\n",
              "3  Le Castella Viet Nam - Bánh Bông Lan Đài Loan ...  ...  [[thấy, bánh, hot, rần_rần, mình, gọi, thử, hộ...\n",
              "4                      Tabletop - Boardgame & Coffee  ...  [[lần, nghĩ, tới, boardgame, nghĩ, tới, vô], [...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7e6afa23-c63b-4661-cb5e-28bc0c1c7933",
        "id": "Tu_iZrhe8GDu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "dict_file = \"drive/My Drive/Master/JVN/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/dict.txt\"\n",
        "lda_6_topics = \"drive/My Drive/Master/JVN/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/6_topics/lda_model\"\n",
        "\n",
        "dictionary = load_dict(dict_file)\n",
        "lda_model_tfidf = load_lda_model(lda_6_topics)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKvJlcS___z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all_sentences = []\n",
        "# all_cleand_sentences = []\n",
        "# count = 0\n",
        "# for index, row in data.iterrows():\n",
        "#   sentences = data.loc[index, sentences_col]\n",
        "#   for sentence in sentences:\n",
        "#     all_sentences.append(sentence)\n",
        "  \n",
        "#   cleaned_sentences = data.loc[index, sentences_cleaned_col]\n",
        "#   for sentence in cleaned_sentences:\n",
        "#     all_cleand_sentences.append(sentence)\n",
        "# print(\"Number of sentences: {}\".format(len(all_sentences)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sfAUc2_-8GDQ",
        "colab": {}
      },
      "source": [
        "# Get documents\n",
        "# n_docs = 948330\n",
        "# documents = collect_tokens(data, n_docs)\n",
        "\n",
        "# n_sampled_docs = 10\n",
        "# print(\"Sample {} in {} docs\".format(n_sampled_docs, len(documents)))\n",
        "# doc_sampled_indices = npr.randint(0, high=len(documents), size=n_sampled_docs)\n",
        "# for index in doc_sampled_indices:\n",
        "#   print(documents[index])\n",
        "\n",
        "# dictionary = Dictionary(documents)\n",
        "# bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "# tfidf_model = TfidfModel(bow_corpus)\n",
        "# corpus_tfidf = tfidf_model[bow_corpus]\n",
        "\n",
        "# # Run LDA model\n",
        "# n_topics = 6\n",
        "# lda_model_tfidf = LdaMulticore(corpus_tfidf, num_topics=n_topics, id2word=dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e949cc4f-0f72-493c-9b63-09e00d2fe193",
        "id": "VEKM8HYQ8GC0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "for index, topic in lda_model_tfidf.print_topics(-1, num_words=20):\n",
        "  print('Topic: {} Word: {}'.format(index, topic))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 Word: 0.034*\"trà\" + 0.026*\"sữa\" + 0.016*\"ngon\" + 0.015*\"uống\" + 0.015*\"vị\" + 0.012*\"ngọt\" + 0.011*\"mình\" + 0.010*\"trân_châu\" + 0.010*\"kem\" + 0.009*\"thơm\" + 0.008*\"thích\" + 0.008*\"thạch\" + 0.006*\"lắm\" + 0.006*\"đá\" + 0.006*\"thấy\" + 0.006*\"quá\" + 0.006*\"béo\" + 0.006*\"ở\" + 0.006*\"hơi\" + 0.006*\"loại\"\n",
            "Topic: 1 Word: 0.024*\"xôi\" + 0.014*\"ăn\" + 0.013*\"gà\" + 0.011*\"ngon\" + 0.011*\"thịt\" + 0.008*\"cơm\" + 0.007*\"nướng\" + 0.007*\"món\" + 0.007*\"k\" + 0.006*\"bò\" + 0.006*\"mình\" + 0.005*\"xiên\" + 0.005*\"sốt\" + 0.005*\"mì\" + 0.005*\"kèm\" + 0.005*\"gọi\" + 0.005*\"chiên\" + 0.005*\"bánh\" + 0.005*\"pizza\" + 0.005*\"cay\"\n",
            "Topic: 2 Word: 0.030*\"nhân_viên\" + 0.023*\"phục_vụ\" + 0.018*\"nhiệt_tình\" + 0.012*\"nhanh\" + 0.011*\"thân_thiện\" + 0.009*\"khách\" + 0.009*\"quán\" + 0.008*\"nhanh_nhẹn\" + 0.008*\"mình\" + 0.008*\"quay\" + 0.008*\"đông\" + 0.008*\"lâu\" + 0.007*\"dễ_thương\" + 0.007*\"khá\" + 0.006*\"đồ\" + 0.006*\"tốt\" + 0.006*\"chủ\" + 0.006*\"xe\" + 0.006*\"vui_vẻ\" + 0.006*\"order\"\n",
            "Topic: 3 Word: 0.020*\"gian\" + 0.016*\"quán\" + 0.016*\"đẹp\" + 0.014*\"tầng\" + 0.012*\"ngồi\" + 0.009*\"khá\" + 0.008*\"nhỏ\" + 0.008*\"nằm\" + 0.008*\"thoáng\" + 0.008*\"tìm\" + 0.008*\"chỗ\" + 0.008*\"dễ\" + 0.007*\"trang_trí\" + 0.007*\"nhìn\" + 0.007*\"view\" + 0.007*\"rộng_rãi\" + 0.007*\"rộng\" + 0.006*\"bàn\" + 0.006*\"mát\" + 0.006*\"ở\"\n",
            "Topic: 4 Word: 0.011*\"lần\" + 0.010*\"đi\" + 0.010*\"mình\" + 0.010*\"quán\" + 0.010*\"đến\" + 0.009*\"ghé\" + 0.009*\"thử\" + 0.008*\"mua\" + 0.007*\"mới\" + 0.006*\"về\" + 0.006*\"bạn\" + 0.006*\"ăn\" + 0.005*\"trà\" + 0.005*\"sữa\" + 0.005*\"đông\" + 0.005*\"ở\" + 0.005*\"thấy\" + 0.004*\"tới\" + 0.004*\"h\" + 0.004*\"dịp\"\n",
            "Topic: 5 Word: 0.031*\"giá\" + 0.016*\"k\" + 0.013*\"đồ\" + 0.012*\"rẻ\" + 0.011*\"ăn\" + 0.011*\"uống\" + 0.011*\"chất_lượng\" + 0.011*\"ổn\" + 0.010*\"ngon\" + 0.009*\"khá\" + 0.008*\"mình\" + 0.007*\"cao\" + 0.007*\"món\" + 0.007*\"ok\" + 0.007*\"đắt\" + 0.006*\"chung\" + 0.006*\"nói_chung\" + 0.006*\"no\" + 0.006*\"thấy\" + 0.006*\"tạm\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "301edf90-3273-4e38-aa25-267f3332a905",
        "id": "NkTBhiq38GCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# save_dict(dictionary, dict_file)\n",
        "# save_lda_model(lda_model_tfidf, lda_6_topics)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eBkg1ie48GCN",
        "colab": {}
      },
      "source": [
        "topic_mapping_6 = {\n",
        "    0: \"Đánh giá thức uống\",\n",
        "    1: \"Đánh giá món ăn\",\n",
        "    2: \"Phục vụ, nhân viên\",\n",
        "    3: \"Không gian\",\n",
        "    4: \"Topic 4\",\n",
        "    5: \"Giá, chất lượng\"\n",
        "}\n",
        "\n",
        "topic_col = \"topic\"\n",
        "review_df = pd.DataFrame(columns=[review_col, topic_col])\n",
        "n_sampled_sentences = 1000\n",
        "# print(\"Sample {} in {} sentences\".format(n_sampled_sentences, len(all_sentences)))\n",
        "sentence_sampled_indices = npr.randint(0, high=len(all_sentences), size=n_sampled_sentences)\n",
        "for index in sentence_sampled_indices:\n",
        "#   print(\"\\nSentence: {}\".format(all_sentences[index]))\n",
        "  topic = \"EMPTY\"\n",
        "  sentence_cleaned = all_cleand_sentences[index]\n",
        "#   print(\"S cleaned: {}\".format(sentence_cleaned))\n",
        "  if sentence_cleaned != \"\":\n",
        "    tokens = tokenizer.tokenize(sentence_cleaned)\n",
        "    topic_dist = lda_model_tfidf.get_document_topics(dictionary.doc2bow(tokens))\n",
        "    topic_id = rank_topic(topic_dist)\n",
        "    topic = get_topic(topic_mapping_6, topic_id)\n",
        "  review_df = review_df.append({review_col: all_sentences[index], topic_col: topic}, ignore_index=True)\n",
        "#   print(\"Topic: {}\".format(topic))\n",
        "review_df.to_csv(\"/content/drive/My Drive/Master/2nd Semester/AdvMLDM/Project_AdvMLDM/dang/lda/review_aspect.csv\"\n",
        "                 , sep=';', header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}